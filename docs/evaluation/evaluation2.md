### Sparsity Evaluation
Another way of assessing the performance of the SAE is by making a density histogram of the sparsity vs. the average activation values of each neuron. As per [Anothropic](https://transformer-circuits.pub/2023/monosemantic-features#appendix-feature-density), a well trained SAE might learn to represent datasets with a mix of low and high density features corresponding to image entropy, which can be seen in the density plot of Hugo Fry's model, where two distinct density bands are present. My understanding is that neurons that represent features in a low entropy class labels fire frequently within that class, while those that capture features in high entropy class label (lots of variation means that a feature might not be common in that class) group in a low frequency cluster. My histogram does not necessarily show that pattern outside of a vague tendency for high activation to have high frequencies. I hypothesize this is a mix of having a  smaller dataset than Hugo and Anthropic (no need to superimpose low and high entropy features to compress images), limited compute budget and time to optimize hyperparameters, and a generally smaller model as well. Interestingly, the highest average activating neuron loosely correponds too... pianos?