FVU (fraction of variance unexplained) is worse than Hugo Fry (who had 14%) and likely contributes to some of the neurons having weak intuitive feature explanations. The loss could have been optimized further with more epochs, and having compute to explore different hyperparameter settings could have possibly achieved a comparable FVU to Hugo. Interestingly, the relatively low auxilliary loss suggests that most features can be effectively reconstructed using the topK "dead" features (haven't been activated in #_threshold batches), which suggests that there might be siginficant overlap among features between the neurons (which can be subjectively verified, because there are many neurons that correponds to "water" or "fish", but few class labels that correspond to these things in imagenet-mini). This suggests that neurons either 1) repeated feature representations or 2) learn to superimpose between themselves for better reconstruction. As another measure of sparsity I measured the # of neurons that that featured in any one the images top 16 activating neurons in imagenet-mini, and model 3 only had ~25% (vs model 2's 50%). This doesn't really make sense, because increasing k would theoretically increasing competition among activations (and prevent convergence to a subset of feautres for representation). This is something to look into further. 
Finally, I evaluated downstream ablation effects on CLIP on a small dataset of (text, image) pairs comparing the cosine similarity between pairs with and without the SAE inserted into the residual stream (layer 11). With more time I would quantify the contrsative loss with accuracy, precision, recall etc., but the 2d histogram suffices to show mild ablation effects with the maintenence of representation power. 